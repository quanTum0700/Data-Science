{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š EDA Template - æ•°æ®æ¢ç´¢ä¸é¢„å¤„ç†æ ‡å‡†æµç¨‹\n",
    "\n",
    "æ¯æ¬¡æ–°é¡¹ç›®/æ¯”èµ›æ—¶å¤åˆ¶æ­¤æ¨¡æ¿ï¼ŒæŒ‰éœ€ä¿®æ”¹ä½¿ç”¨ã€‚\n",
    "\n",
    "**ä½¿ç”¨æ–¹æ³•ï¼š**\n",
    "1. ä¿®æ”¹ Section 1 çš„æ–‡ä»¶è·¯å¾„\n",
    "2. ä¿®æ”¹ Section 5 çš„ `TARGET` å˜é‡å\n",
    "3. é€ä¸ª Cell è¿è¡Œï¼ŒæŒ‰éœ€è°ƒæ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. å¯¼å…¥å¸¸ç”¨åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# æ˜¾ç¤ºè®¾ç½®\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('âœ… Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. åŠ è½½æ•°æ®\n",
    "\n",
    "âš ï¸ **TODO: ä¿®æ”¹æ–‡ä»¶è·¯å¾„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ä¿®æ”¹æ–‡ä»¶è·¯å¾„\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. å¿«é€Ÿæ¦‚è§ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_overview(df, name=\"DataFrame\"):\n",
    "    \"\"\"æ•°æ®é›†å¿«é€Ÿæ¦‚è§ˆ\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š {name} Overview\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"\\n--- Data Types ---\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_overview(train, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_overview(test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯¦ç»†ä¿¡æ¯\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¼ç»Ÿè®¡\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç±»åˆ«ç»Ÿè®¡\n",
    "train.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ç¼ºå¤±å€¼åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_analysis(df, name=\"DataFrame\"):\n",
    "    \"\"\"ç¼ºå¤±å€¼åˆ†æ\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = df.isnull().mean() * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” {name} Missing Values\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if len(missing_df) == 0:\n",
    "        print(\"âœ… No missing values!\")\n",
    "    else:\n",
    "        print(f\"Columns with missing values: {len(missing_df)}\")\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing = missing_analysis(train, \"Train\")\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing = missing_analysis(test, \"Test\")\n",
    "test_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. é‡å¤å€¼æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_check(df, name=\"DataFrame\"):\n",
    "    \"\"\"é‡å¤å€¼æ£€æŸ¥\"\"\"\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"ğŸ”„ {name} Duplicates: {dup_count} ({dup_count/len(df)*100:.2f}%)\")\n",
    "    return dup_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dups = duplicate_check(train, \"Train\")\n",
    "test_dups = duplicate_check(test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚éœ€åˆ é™¤é‡å¤å€¼ï¼Œå–æ¶ˆä¸‹è¡Œæ³¨é‡Š\n",
    "# train = train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ç›®æ ‡å˜é‡åˆ†æ\n",
    "\n",
    "âš ï¸ **TODO: ä¿®æ”¹ç›®æ ‡å˜é‡å**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ä¿®æ”¹ç›®æ ‡å˜é‡å\n",
    "TARGET = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å€¼åˆ†å¸ƒ\n",
    "print(\"--- Value Counts ---\")\n",
    "print(train[TARGET].value_counts())\n",
    "print(\"\\n--- Value Proportions ---\")\n",
    "print(train[TARGET].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®æ ‡å˜é‡å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# æŸ±çŠ¶å›¾\n",
    "train[TARGET].value_counts().plot(kind='bar', ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title(f'{TARGET} Distribution')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# é¥¼å›¾\n",
    "train[TARGET].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[1])\n",
    "axes[1].set_title(f'{TARGET} Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ç‰¹å¾ç±»å‹åˆ†ç¦»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features(df, target_col=None):\n",
    "    \"\"\"åˆ†ç¦»æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾\"\"\"\n",
    "    cols = [c for c in df.columns if c != target_col]\n",
    "    \n",
    "    num_cols = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df[cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numeric features: {len(num_cols)}\")\n",
    "    print(f\"Categorical features: {len(cat_cols)}\")\n",
    "    \n",
    "    return num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols, cat_cols = separate_features(train, TARGET)\n",
    "\n",
    "print(f\"\\nNumeric: {num_cols}\")\n",
    "print(f\"\\nCategorical: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. æ•°å€¼ç‰¹å¾åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_distribution(df, num_cols, n_cols=4):\n",
    "    \"\"\"æ•°å€¼ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–\"\"\"\n",
    "    if len(num_cols) == 0:\n",
    "        print(\"No numeric features to analyze.\")\n",
    "        return\n",
    "    \n",
    "    n_features = min(len(num_cols), 16)  # æœ€å¤šæ˜¾ç¤º16ä¸ª\n",
    "    n_rows = (n_features - 1) // n_cols + 1\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(num_cols[:n_features]):\n",
    "        df[col].hist(ax=axes[i], bins=30, edgecolor='black')\n",
    "        axes[i].set_title(col)\n",
    "    \n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_distribution(train, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ç±»åˆ«ç‰¹å¾åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_summary(df, cat_cols):\n",
    "    \"\"\"ç±»åˆ«ç‰¹å¾å”¯ä¸€å€¼ç»Ÿè®¡\"\"\"\n",
    "    if len(cat_cols) == 0:\n",
    "        print(\"No categorical features to analyze.\")\n",
    "        return None\n",
    "    \n",
    "    cat_info = pd.DataFrame({\n",
    "        'Unique Values': [df[col].nunique() for col in cat_cols],\n",
    "        'Sample Values': [df[col].dropna().unique()[:3].tolist() for col in cat_cols]\n",
    "    }, index=cat_cols).sort_values('Unique Values', ascending=False)\n",
    "    \n",
    "    return cat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_info = categorical_summary(train, cat_cols)\n",
    "cat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ç›¸å…³æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›¸å…³æ€§çŸ©é˜µçƒ­åŠ›å›¾\n",
    "if len(num_cols) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = train[num_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§ (ä»…å½“ç›®æ ‡æ˜¯æ•°å€¼å‹æ—¶)\n",
    "if TARGET in train.columns and train[TARGET].dtype in [np.number, 'int64', 'float64']:\n",
    "    correlations = train[num_cols + [TARGET]].corr()[TARGET].drop(TARGET)\n",
    "    correlations = correlations.sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"Correlation with {TARGET}:\")\n",
    "    print(correlations)\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    plt.figure(figsize=(10, max(6, len(correlations) * 0.3)))\n",
    "    correlations.plot(kind='barh')\n",
    "    plt.title(f'Feature Correlation with {TARGET}')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. æ•°æ®åˆ’åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(TARGET, axis=1)\n",
    "y = train[TARGET]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨ï¼Œå›å½’ä»»åŠ¡åˆ é™¤æ­¤å‚æ•°\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"\\ny_train distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\ny_val distribution:\")\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. å¸¸ç”¨é¢„å¤„ç†å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, num_strategy='median', cat_strategy='mode'):\n",
    "    \"\"\"å¡«å……ç¼ºå¤±å€¼\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        num_strategy: 'median' | 'mean' | 'zero'\n",
    "        cat_strategy: 'mode' | 'unknown'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                if num_strategy == 'median':\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "                elif num_strategy == 'mean':\n",
    "                    df[col].fillna(df[col].mean(), inplace=True)\n",
    "                elif num_strategy == 'zero':\n",
    "                    df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                if cat_strategy == 'mode':\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "                elif cat_strategy == 'unknown':\n",
    "                    df[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df, cat_cols, encoding='label'):\n",
    "    \"\"\"ç¼–ç ç±»åˆ«ç‰¹å¾\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        cat_cols: ç±»åˆ«åˆ—ååˆ—è¡¨\n",
    "        encoding: 'label' | 'onehot'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if encoding == 'label':\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    elif encoding == 'onehot':\n",
    "        df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹ï¼ˆæŒ‰éœ€å–æ¶ˆæ³¨é‡Šï¼‰\n",
    "# train_processed = fill_missing(train)\n",
    "# train_processed = encode_categorical(train_processed, cat_cols, encoding='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. å¯¼å‡ºæ•°æ®ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å‡ºå¤„ç†åçš„æ•°æ®\n",
    "# train_processed.to_csv('train_processed.csv', index=False)\n",
    "# test_processed.to_csv('test_processed.csv', index=False)\n",
    "# print('âœ… Data exported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… EDA Complete!\n",
    "\n",
    "**ä¸‹ä¸€æ­¥ï¼š**\n",
    "- [ ] ç‰¹å¾å·¥ç¨‹\n",
    "- [ ] å»ºç«‹ Baseline æ¨¡å‹\n",
    "- [ ] äº¤å‰éªŒè¯\n",
    "- [ ] æ¨¡å‹è°ƒä¼˜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
